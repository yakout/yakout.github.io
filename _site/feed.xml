<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-09-08T15:46:06+02:00</updated><id>http://localhost:4000/</id><title type="html">Ahmed Yakout</title><subtitle>My blog.</subtitle><author><name>Ahmed Yakout</name><email>iyakout@hotmail.com</email></author><entry><title type="html">Coursera Deep learning [Week 4]</title><link href="http://localhost:4000/deeplearning/deep-learning-Week-4/" rel="alternate" type="text/html" title="Coursera Deep learning [Week 4]" /><published>2017-09-04T00:00:00+02:00</published><updated>2016-11-03T17:45:04+02:00</updated><id>http://localhost:4000/deeplearning/deep-learning-%5BWeek-4%5D</id><content type="html" xml:base="http://localhost:4000/deeplearning/deep-learning-Week-4/">&lt;p&gt;Steps for training a neural network (Summery)&lt;/p&gt;

&lt;aside class=&quot;sidebar__right&quot;&gt;
&lt;nav class=&quot;toc&quot;&gt;
    &lt;header&gt;&lt;h4 class=&quot;nav__title&quot;&gt;&lt;i class=&quot;fa fa-file-text&quot;&gt;&lt;/i&gt; Steps&lt;/h4&gt;&lt;/header&gt;
&lt;ul class=&quot;toc__menu&quot; id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#step-1-initialize-parameters&quot; id=&quot;markdown-toc-step-1-initialize-parameters&quot;&gt;Step 1: Initialize parameters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-2-implement-forward-propagation&quot; id=&quot;markdown-toc-step-2-implement-forward-propagation&quot;&gt;Step 2: Implement forward propagation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-3-compute-cost-function-j&quot; id=&quot;markdown-toc-step-3-compute-cost-function-j&quot;&gt;Step 3: Compute cost function (J)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-4-implement-backward-propagation&quot; id=&quot;markdown-toc-step-4-implement-backward-propagation&quot;&gt;Step 4: Implement backward propagation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-5-update-parameters&quot; id=&quot;markdown-toc-step-5-update-parameters&quot;&gt;Step 5: Update parameters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-6-put-step-2-5-into-for-loop-num_iterations-gradient-descent&quot; id=&quot;markdown-toc-step-6-put-step-2-5-into-for-loop-num_iterations-gradient-descent&quot;&gt;Step 6: put step 2-5 into for loop num_iterations (gradient descent)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-7-predict&quot; id=&quot;markdown-toc-step-7-predict&quot;&gt;Step 7: Predict&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

  &lt;/nav&gt;
&lt;/aside&gt;

&lt;h2 id=&quot;step-1-initialize-parameters&quot;&gt;Step 1: Initialize parameters&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The bias vectors are initialized with zeros.&lt;/li&gt;
  &lt;li&gt;The weights matrices are initialized with small random variables
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;err&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;err&quot;&gt; &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;0.01&lt;/code&gt; can also be a variable that is chosen later but it’s not widely common. 
  Choosing a big constant will affect the speed of gradient descent algorithm in some activation function like &lt;code class=&quot;highlighter-rouge&quot;&gt;tanh(z)&lt;/code&gt; so the values will be either very small or very big and hence the gradients will be close to zero and will slow the algorithm.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
	&lt;a href=&quot;/assets/images/dl-tanh.png&quot;&gt;&lt;img src=&quot;/assets/images/dl-tanh.png&quot; /&gt;&lt;/a&gt;
	&lt;figcaption&gt;tanh(z)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;step-2-implement-forward-propagation&quot;&gt;Step 2: Implement forward propagation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;compute Z&lt;sup&gt;l&lt;/sup&gt; and A&lt;sup&gt;l&lt;/sup&gt; for all layers&lt;/li&gt;
  &lt;li&gt;linear forward (compute Z)&lt;/li&gt;
  &lt;li&gt;activation forward (compute A)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;step-3-compute-cost-function-j&quot;&gt;Step 3: Compute cost function (J)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;this is not included in the computations but it’s very useful for debugging and visualization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;step-4-implement-backward-propagation&quot;&gt;Step 4: Implement backward propagation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use cached values from forward propagation to compute the gradients&lt;/li&gt;
  &lt;li&gt;Linear backward (compute dW, db, dA_prev)&lt;/li&gt;
  &lt;li&gt;Activation backward (com)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;step-5-update-parameters&quot;&gt;Step 5: Update parameters&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Update the parameters using the gradients computer in step 4.
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  theta = theta - alpha * dtheta
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
    &lt;p&gt;Where &lt;code class=&quot;highlighter-rouge&quot;&gt;alpha&lt;/code&gt; is the learning_rate and &lt;code class=&quot;highlighter-rouge&quot;&gt;dtheta&lt;/code&gt; is the derivative of cost function &lt;code class=&quot;highlighter-rouge&quot;&gt;J&lt;/code&gt; with respect to theta.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;step-6-put-step-2-5-into-for-loop-num_iterations-gradient-descent&quot;&gt;Step 6: put step 2-5 into for loop num_iterations (gradient descent)&lt;/h2&gt;

&lt;h2 id=&quot;step-7-predict&quot;&gt;Step 7: Predict&lt;/h2&gt;

&lt;figure&gt;
	&lt;a href=&quot;/assets/images/final_outline.png&quot;&gt;&lt;img src=&quot;/assets/images/final_outline.png&quot; /&gt;&lt;/a&gt;
	&lt;figcaption&gt;(The model's structure is [LINEAR -&amp;gt; RELU] × (L-1) -&amp;gt; LINEAR -&amp;gt; SIGMOID. I.e., it has L−1 layers using a ReLU activation function followed by an output layer with a sigmoid activation function.)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;hr /&gt;

&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;we don’t calculate the input layer in the total number of NN layers&lt;/li&gt;
  &lt;li&gt;The input layer is layer zero (l = 0)&lt;/li&gt;
  &lt;li&gt;A0 = X&lt;/li&gt;
  &lt;li&gt;n0 = n_x&lt;/li&gt;
  &lt;li&gt;AL = Y^ (Y-hat)&lt;/li&gt;
  &lt;li&gt;1 layer NN is actually logistic regression (shallow NN)&lt;/li&gt;
  &lt;li&gt;More than 2 layer NN is called (Deep NN)&lt;/li&gt;
  &lt;li&gt;In deep learning, the “[LINEAR-&amp;gt;ACTIVATION]” (compute the forward linear step followed by forward activation step) computation is counted as a single layer in the neural network, not two layers.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Ahmed Yakout</name><email>iyakout@hotmail.com</email></author><category term="knowledge" /><summary type="html">Steps for training a neural network (Summery)</summary></entry></feed>